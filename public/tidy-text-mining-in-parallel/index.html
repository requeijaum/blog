<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.32.2" />


<title>tidy text mining in parallel - Paul Oldham&#39;s Blog</title>
<meta property="og:title" content="tidy text mining in parallel - Paul Oldham&#39;s Blog">
<meta property="og:type" content="article">


  <meta name="twitter:card" content="summary">
  <meta name="twitter:image" content="https://www.pauloldham.net/images/paul-oval.png" >
  


<meta property="description" content="Article posted by Paul Oldham, on Wednesday, January 10nd, 2018">
<meta property="og:description" content="Article posted by Paul Oldham, on Wednesday, January 10nd, 2018">



<meta name="twitter:creator" content="@junglepaul">
<meta name="twitter:site" content="">


  








<link href='//cdn.bootcss.com/highlight.js/9.11.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/paul-oval.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="https://twitter.com/junglepaul">@junglepaul</a></li>
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/poldham">GitHub</a></li>
    
    <li><a href="https://github.com/wipo-analytics">WIPO Analytics</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">5 min read</span>
    

    <h1 class="article-title">tidy text mining in parallel</h1>

    
    <span class="article-date">2018/01/10</span>
    

    <div class="article-content">
      <p>I have been involved with text mining for quite a few years and I am a big fan of tidy text mining in R as popularised by Julia Silge and Daniel Robinson in <a href="https://www.tidytextmining.com/">Text Mining in R: A Tidy Approach</a>. What I really like about tidy text mining is that we can keep track of the files where a word, a sentence or paragraph come from. This really matters when we want to join the results of tidy text mining to other data such as taxonomic information on species or to maps, or both. The transparency of the tidytext approach, when compared for example with machine learning approaches, also really appeals to me in terms of reproducibility and understandibility.</p>
<p>One of the big constraints with text mining in R (or Python) is that everything runs on a single core. This can result in long waits or crashes when working at the scale of a few thousand documents or more. So, I have been investigating possible solutions for text mining in parallel in R.</p>
<p>The story so far. I will try and highlight some useful tutorials that you may want to follow up on.</p>
<ul>
<li>doParallel <a href="http://blog.aicry.com/r-parallel-computing-in-5-minutes/">walk through</a></li>
<li>parallel</li>
<li>partools <a href="https://matloff.wordpress.com/2015/08/05/partools-a-sensible-r-package-for-large-data-sets/">walk through</a></li>
<li><a href="https://github.com/vertica/DistributedR">distributedR</a> from Vertica Analytics</li>
<li>foreach</li>
<li>sparklyr <a href="https://spark.rstudio.com/guides/textmining/">walk through</a></li>
<li>multidplyr</li>
</ul>
<p>In this post I’m going to use Hadley Wickham’s <a href="https://github.com/hadley/multidplyr">multidplyr</a> package for parallesing tidyverse code… since its a tidy approach that I am looking for. In exploring the options for parallelising text mining I’ve also found that this is an easy way in to exploring the issues and it works.</p>
<div id="getting-started" class="section level2">
<h2>Getting Started</h2>
<p><a href="https://github.com/hadley/multidplyr">multidplyr</a> has not received a lot of attention compared with other tidyverse related packages and the last commit on Github was a year ago. This raises the question of whether HW is adopting an “if it ain’t broke don’t fix it” approach or whether it is unlikely to go further. Only Hadley can answer that question.</p>
<p>multidplyr is not on CRAN so we need to install from github</p>
<pre class="r"><code># install.packages(&quot;devtools&quot;)
devtools::install_github(&quot;hadley/multidplyr&quot;)</code></pre>
<p>We will also be needing some other packages</p>
<pre class="r"><code>install.packages(&quot;tidytext&quot;)
install.packages(&quot;tidyverse&quot;)
install.packages(&quot;readtext&quot;)</code></pre>
<pre class="r"><code>library(multidplyr)
library(tidytext)
library(tidyverse)
library(readtext)</code></pre>
<p>Let’s work with some actual texts to illustrate. One challenge with R can be finding an easy way to read in texts… in this case a zipped file with a bunch of text files without unzipping the file first (as it may be huge). We can do this easily with <a href="https://github.com/quanteda/readtext"><code>readtext</code></a> by Kenneth Benoit at my alma mater, the LSE.</p>
<pre class="r"><code>readtext::readtext()</code></pre>
<p>If you receive an error through mixed encoding in the files the encoding argument will frequently address problems e.g encoding = “UTF8” and display a helpful message for the problem files. We are using text files here but the function also handles xml, pdf and word documents and is a good allround function.</p>
<div id="a-cluster-in-one-line" class="section level3">
<h3>A cluster in one line</h3>
<p>Multidplyr involves setting up a cluster and partitioning a data.frame into a <code>party_df</code> before assigning packages, values and expressions to the nodes.</p>
<p>The first part of this process can be handled in one line with a set of inbuilt defaults,</p>
<pre class="r"><code>library(multidplyr)
test &lt;- partition(full_texts)</code></pre>
<p>If we now take a look at test2 we find that it is a <code>party_df</code></p>
<pre class="r"><code>class(test)</code></pre>
<p>Behind the scenes the call to partition creates a default cluster (7 nodes) and randomly assigns the rows to groups across the nodes in the cluster. The code below shows the default arguments with the call to cluster generating a default cluster with 7 nodes.</p>
<pre class="r"><code>partition(.data, ..., cluster = get_default_cluster())</code></pre>
<p>The next step involves assigning packages to the cluster and values or expressions. We can do this using the pipe.</p>
<pre class="r"><code>test &lt;- partition(full_texts) %&gt;%
  cluster_library(c(&quot;tidyverse&quot;, &quot;tidytext&quot;))</code></pre>
<p>So, to get started with multidplyr we simply need the following lines of code.</p>
<pre class="r"><code>test</code></pre>
<p>So, it is remarkably easy to set up. Next we need something to text mine the texts for. Here we will use a vector of country names.</p>
<p>THERE IS SOMETHING WRONG WITH THIS AS ONLY SHOWING 422 results suggesting it is only finding the first of the terms not all of them</p>
<pre class="r"><code>country &lt;- c(&quot;brunei|cambodia|burma|indonesia|laos|lao|malaysia|philippines|singapore|thailand|vietnam&quot;)</code></pre>
<p>We now pass this object to the nodes using <code>cluster_assign_value</code></p>
<pre class="r"><code>test &lt;- partition(full_texts) %&gt;%
  cluster_library(c(&quot;tidyverse&quot;, &quot;tidytext&quot;)) %&gt;%
  cluster_assign_value(&quot;country&quot;, country)</code></pre>
<p>We can now do a quick bit of text mining on the cluster. The question we will ask here is which of the documents contain one of our country names. We will use <code>str_detect</code> from <code>stringr</code> which will return a logical value that we will place in a new column called country. At the end of the process we will collect the results into a new data frame.</p>
<pre class="r"><code>start &lt;- proc.time() # start timer
test2 &lt;- test %&gt;%
  mutate(text, country = str_detect(text, country)) %&gt;% 
  collect()
time_taken &lt;- proc.time() - start # calculate time
time_taken</code></pre>
<p>Having loaded in our sample texts we will want to set up our local cluster. There are a number of steps to this and I am following this really useful post by <a href="http://www.business-science.io/code-tools/2016/12/18/multidplyr.html">Matt Dancho</a> as the most useful of the walkthroughs I have found so far. You might also find this blog post by <a href="http://blog.aicry.com/multidplyr-dplyr-meets-parallel-processing/index.html">Adam Harasimowicz</a> useful.</p>
<p>As explained in Matt Dancho’s post and the package vignette the basic steps are</p>
<ol style="list-style-type: decimal">
<li>Find out how many cores you have on your machine</li>
<li>Set up a local cluster</li>
<li>Partitition your data frame onto the cluster (a grouped_df)</li>
<li>Assign packages, functions, values and expressions to the nodes in the cluster</li>
<li>Run your code and collect the data</li>
</ol>
<p>Step 1: How many cores</p>
<p>Step 2: Set up a local cluster</p>
<p>You can use as many cores as you have available. But if you use all of them bear in mind that the machine may grind to a halt.</p>
<pre class="r"><code>cluster &lt;- multidplyr::create_cluster(cores = 6)</code></pre>
<p>Step 3: Partition your data frame</p>
<p>Step 4: Assign Packages, Functions, Values and Expressions</p>
<p>You can chain the assignments together here we will break them up a bit</p>
<pre class="r"><code>groupeddf %&gt;%
    multidplyr::cluster_library(&quot;tidyverse&quot;) %&gt;%
    multidplyr::cluster_library(&quot;tidytext&quot;)</code></pre>
<p>It is often helpful to assign values to the nodes on the cluster</p>
<pre class="r"><code> groupeddf %&gt;%
    multidplyr::cluster_assign_value(value, as.name(value))</code></pre>
</div>
<div id="a-simpler-approach" class="section level3">
<h3>A simpler approach</h3>
<p>In practice <code>multidplyr</code> has some sensible defaults that makes using it more accessible.</p>
<p>We can combine the partition step and the cluster step into one line as follows. Note here that behind the scenes the call to partition will randomly partition the rows across nodes after creating a default cluster with 7 nodes.</p>
<pre class="r"><code>test &lt;- partition(full_texts, cluster = get_default_cluster())</code></pre>
</div>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdn.bootcss.com/highlight.js/9.11.0/highlight.min.js"></script>



<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/r.min.js"></script>
<script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    

    
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-113668064-1', 'auto');
ga('send', 'pageview');
</script>

  </body>
</html>

