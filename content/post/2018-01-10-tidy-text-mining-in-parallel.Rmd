---
title: tidy text mining in parallel
author: Paul Oldham
date: '2018-01-10'
draft: true
slug: tidy-text-mining-in-parallel
categories:
  - R
tags:
  - tidytext
  - text mining
  - multidplyr
---
```{r opts, echo=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, fig.align = "center", out.width = "600px", eval = FALSE)
```

I have been involved with text mining for quite a few years and I am a big fan of tidy text mining in R as popularised by Julia Silge and Daniel Robinson in [Text Mining in R: A Tidy Approach](https://www.tidytextmining.com/). What I really like about tidy text mining is that we can keep track of the files where a word, a sentence or paragraph come from. This really matters when we want to join the results of tidy text mining to other data such as taxonomic information on species or to maps, or both. The transparency of the tidytext approach, when compared for example with machine learning approaches, also really appeals to me in terms of reproducibility and understandibility. 

One of the big constraints with text mining in R (or Python) is that everything runs on a single core. This can result in long waits or crashes when working at the scale of a few thousand documents or more. So, I have been investigating possible solutions for text mining in parallel in R. 

The story so far. I will try and highlight some useful tutorials that you may want to follow up on. 

- doParallel [walk through](http://blog.aicry.com/r-parallel-computing-in-5-minutes/)
- parallel
- partools [walk through](https://matloff.wordpress.com/2015/08/05/partools-a-sensible-r-package-for-large-data-sets/)
- [distributedR](https://github.com/vertica/DistributedR) from Vertica Analytics
- foreach 
- sparklyr [walk through](https://spark.rstudio.com/guides/textmining/)
- multidplyr

In this post I'm going to use Hadley Wickham's [multidplyr](https://github.com/hadley/multidplyr) package for parallesing tidyverse code... since its a tidy approach that I am looking for. In exploring the options for parallelising text mining I've also found that this is an easy way in to exploring the issues and it works. 

## Getting Started

[multidplyr](https://github.com/hadley/multidplyr) has not received a lot of attention compared with other tidyverse related packages and the last commit on Github was a year ago. This raises the question of whether HW is adopting an "if it ain't broke don't fix it" approach or whether it is unlikely to go further. Only Hadley can answer that question. 

multidplyr is not on CRAN so we need to install from github

```{r install_github, eval=FALSE}
# install.packages("devtools")
devtools::install_github("hadley/multidplyr")
```

We will also be needing some other packages

```{r install, eval=FALSE}
install.packages("tidytext")
install.packages("tidyverse")
install.packages("readtext")
```

```{r library}
library(multidplyr)
library(tidytext)
library(tidyverse)
library(readtext)
```


Let's work with some actual texts to illustrate. One challenge with R can be finding an easy way to read in texts... in this case a zipped file with a bunch of text files without unzipping the file first (as it may be huge). We can do this easily with [`readtext`](https://github.com/quanteda/readtext) by Kenneth Benoit at my alma mater, the LSE.

```{r readtext}
readtext::readtext()
```

If you receive an error through mixed encoding in the files the encoding argument will frequently address problems e.g encoding = "UTF8" and display a helpful message for the problem files. We are using text files here but the function also handles xml, pdf and word documents and is a good allround function. 

### A cluster in one line

Multidplyr involves setting up a cluster and partitioning a data.frame into a `party_df` before assigning packages, values and expressions to the nodes. 

The first part of this process can be handled in one line with a set of inbuilt defaults, 

```{r}
library(multidplyr)
test <- partition(full_texts)
```

If we now take a look at test2 we find that it is a `party_df`

```{r}
class(test)
```

Behind the scenes the call to partition creates a default cluster (7 nodes) and randomly assigns the rows to groups across the nodes in the cluster. The code below shows the default arguments with the call to cluster generating a default cluster with 7 nodes. 

```{r underneath, eval=FALSE}
partition(.data, ..., cluster = get_default_cluster())
```

The next step involves assigning packages to the cluster and values or expressions. We can do this using the pipe. 

```{r}
test <- partition(full_texts) %>%
  cluster_library(c("tidyverse", "tidytext"))
```

So, to get started with multidplyr we simply need the following lines of code. 

```{r}
test
```



So, it is remarkably easy to set up. Next we need something to text mine the texts for. Here we will use a vector of country names. 
 
 THERE IS SOMETHING WRONG WITH THIS AS ONLY SHOWING 422 results suggesting it is only finding the first of the terms not all of them
```{r}
country <- c("brunei|cambodia|burma|indonesia|laos|lao|malaysia|philippines|singapore|thailand|vietnam")
```

We now pass this object to the nodes using `cluster_assign_value`

```{r}
test <- partition(full_texts) %>%
  cluster_library(c("tidyverse", "tidytext")) %>%
  cluster_assign_value("country", country)
```

We can now do a quick bit of text mining on the cluster. The question we will ask here is which of the documents contain one of our country names. We will use `str_detect` from `stringr` which will return a logical value that we will place in a new column called country. At the end of the process we will collect the results into a new data frame.  

```{r mine}
start <- proc.time() # start timer
test2 <- test %>%
  mutate(text, country = str_detect(text, country)) %>% 
  collect()
time_taken <- proc.time() - start # calculate time
time_taken
```



Having loaded in our sample texts we will want to set up our local cluster. There are a number of steps to this and I am following this really useful post by [Matt Dancho](http://www.business-science.io/code-tools/2016/12/18/multidplyr.html) as the most useful of the walkthroughs I have found so far. You might also find this blog post by [Adam Harasimowicz](http://blog.aicry.com/multidplyr-dplyr-meets-parallel-processing/index.html) useful. 

As explained in Matt Dancho's post and the package vignette the basic steps are

1. Find out how many cores you have on your machine
1. Set up a local cluster
2. Partitition your data frame onto the cluster (a grouped_df)
3. Assign packages, functions, values and expressions to the nodes in the cluster
4. Run your code and collect the data

Step 1: How many cores
```{r}

```

Step 2: Set up a local cluster

You can use as many cores as you have available. But if you use all of them bear in mind that the machine may grind to a halt.
```{r create_cluster}
cluster <- multidplyr::create_cluster(cores = 6)
```

Step 3: Partition your data frame

```{r}

```

Step 4: Assign Packages, Functions, Values and Expressions

You can chain the assignments together here we will break them up a bit

```{r}
groupeddf %>%
    multidplyr::cluster_library("tidyverse") %>%
    multidplyr::cluster_library("tidytext")
```

It is often helpful to assign values to the nodes on the cluster


```{r}
 groupeddf %>%
    multidplyr::cluster_assign_value(value, as.name(value))
```

### A simpler approach

In practice `multidplyr` has some sensible defaults that makes using it more accessible. 

We can combine the partition step and the cluster step into one line as follows. Note here that behind the scenes the call to partition will randomly partition the rows across nodes after creating a default cluster with 7 nodes. 

```{r quick cluster and partition}
test <- partition(full_texts, cluster = get_default_cluster())
```

