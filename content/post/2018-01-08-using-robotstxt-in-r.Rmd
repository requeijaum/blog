---
title: Dr. Evil meets the robotstxt package
author: Paul Oldham
date: '2018-01-08'
slug: using-robotstxt-in-r
description: "A quick and mildly evil guide to the @ROpensci robotstxt package #rstats"
twitterImg: "images/austin_powers_gangsta.gif"
categories: [R]
tags: [webscraping, robots, robotstxt, biospolar, ropensci]
---
```{r opts, echo=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, fig.align = "center", out.width = "600px")
```

```{r echo=FALSE}
#knitr::include_graphics("https://media.giphy.com/media/X0sAT1NVHUQHS/giphy.gif")
```

I am fairly new to webscraping in R using [rvest](https://github.com/hadley/rvest) and one question is whether a site gives permission for scraping. This information is often contained in the robots.txt file on a website. So, I'm briefly going to explore the [ROpenSci](https://ropensci.org/) [robotstxt](https://github.com/ropenscilabs/robotstxt) package by [Peter Meissner](https://github.com/petermeissnerpackage). [robotstxt](https://github.com/ropenscilabs/robotstxt) provides easy access to the robots.txt file for a domain from R. 

I'm slowly working on a new R data package for underwater geographic feature names as part of a Norwegian Research Council funded project `biospolar` on innovation involving biodiversity in marine polar areas. One of the main data sources for the package is the [General Bathymetric Chart of the Oceans or GEBCO Gazeteer](https://www.gebco.net/data_and_products/undersea_feature_names/). I'm also going to be bringing in data from the [Interridge database of hydrothermal vents](https://vents-data.interridge.org/) and so wanted to understand whether I am just free to go ahead.

The robots.txt content is advisory, and well we could always choose to be Dr. Evil. If my wife would let me have a cat it would definitely be called Mr. Bigglesworth. But it strikes me that building a package for a data source that tries to prohibit scraping might not be a brilliant idea.

There are a bunch of functions in the `robotstxt` package but I'm just going to use the main one `robotstxt()`. Take a look at the [vignette](https://cran.r-project.org/web/packages/robotstxt/vignettes/using_robotstxt.html) for more information. For a very quick check on whether scraping on a path is allowed try the `paths_allowed()` function. I'll come back to that at the end. 

The first place I am going to look is the main GEBCO domain.

```{r}
library(robotstxt)
gebco <- robotstxt("https://www.gebco.net")
gebco
```

This returns a list from the robots txt where the main bit I am interested in is the data frame under gebco$permissions. 

```{r echo=FALSE}
knitr::kable(gebco$permissions)
```

What is of interest here are the entries under Value which can be a bit difficult to interpret. With the help of the handy [Wikipedia article on the Robots Exclusion Standard](https://en.wikipedia.org/wiki/Robots_exclusion_standard) I can see that: 

- `Disallow + *` means to stay out of the website altogether. 

- `Disallow + /xyz` means to stay out of the specific directories.

- `Disallow Googlebot` means that the named bot should stay out of either the website or (as in this case) specific directories. Note that Googlebot appears to be in the naughty seat because the site is more specific about what it should stay out of while others would be free to enter?

However, the GEBCO data files that I am interested in are not hosted on the gebco.net domain but on the [NOAA National Centers for Environmental Information domain](https://www.ngdc.noaa.gov/). 

```{r}
noaa <- robotstxt(domain = "https://www.ngdc.noaa.gov")
noaa
```

The NOAA robotstxt provides some different information. For example, NOAA specifies a crawl delay of 60 seconds which tells me to build in a delay of 60 seconds to a call.  

```{r}
noaa$text
```

We then see a list of disallowed directories and in this case I am interested in the `https://www.ngdc.noaa.gov/gazetteer/`

The dir I am interested in for the package is not on the list so I think I am free to go ahead... yay! 

If I wanted to do this more quickly I would use the `paths_allowed()` function.

```{r}
paths_allowed("https://www.ngdc.noaa.gov/gazetteer/")
```

So, there we have it. If we prefer to be good web scraping citizens rather than the Dr. Evil of web scraping in R then the `robotstxt` package will help us out. On the other hand we could just be evil and see what happens. I'm off to stroke Mr. Bigglesworth. 
